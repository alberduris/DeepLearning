{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN - Tensorflow API - Dynamic - Predicting binary sequences\n",
    "\n",
    "In this notebook we will implement a \"cleaner\" (without explanations, examples, images...) of the last model, the one using the dynamic API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_OBSERVATIONS = 10000\n",
    "\n",
    "NUM_STEPS = 10 #Number of truncated backprop steps\n",
    "BATCH_SIZE = 3\n",
    "NUM_CLASSES = 2 #Binary problem\n",
    "\n",
    "STATE_SIZE = 16\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISPLAY_FREQ = NUM_OBSERVATIONS//NUM_STEPS//BATCH_SIZE//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With current configuration 333 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "print('With current configuration %d batches per epoch' % (NUM_OBSERVATIONS//NUM_STEPS//BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=NUM_OBSERVATIONS):\n",
    "    X = np.array(np.random.choice(2,size=(size,))) #Random binary array of size 'size'\n",
    "    Y = [] #Targets\n",
    "    \n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1: #First dependency at t-3\n",
    "            threshold += 0.5 \n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "            \n",
    "    return X,np.array(Y)\n",
    "\n",
    "def gen_batch(raw_data,batch_size,num_steps):\n",
    "    raw_x,raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "    \n",
    "    #Partition data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length//batch_size\n",
    "    \n",
    "    data_x = np.zeros(shape=(batch_size,batch_partition_length),dtype=np.int32)\n",
    "    data_y = np.zeros(shape=(batch_size,batch_partition_length),dtype=np.int32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length*i : batch_partition_length*(i+1)]\n",
    "        data_y[i] = raw_y[batch_partition_length*i : batch_partition_length*(i+1)]\n",
    "        \n",
    "    #Further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:,i * num_steps:(i+1) * num_steps]\n",
    "        y = data_y[:,i * num_steps:(i+1) * num_steps]\n",
    "        yield(x,y)\n",
    "        \n",
    "def gen_epochs(num_epochs,batch_size,num_steps):\n",
    "    for i in range(num_epochs):\n",
    "        yield gen_batch(gen_data(),batch_size,num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Placeholders\n",
    "\n",
    "x = tf.placeholder(shape=[BATCH_SIZE,NUM_STEPS],dtype=tf.int32,name='input_placeholder')\n",
    "y = tf.placeholder(shape=[BATCH_SIZE,NUM_STEPS],dtype=tf.int32,name='labels_placeholder')\n",
    "init_state = tf.zeros([BATCH_SIZE,STATE_SIZE],dtype=tf.float32)\n",
    "\n",
    "#RNN inputs\n",
    "rnn_inputs = tf.one_hot(x,NUM_CLASSES)\n",
    "\n",
    "#RNN cell\n",
    "cell = tf.contrib.rnn.BasicRNNCell(STATE_SIZE)\n",
    "\n",
    "#Add RNN cells to graph\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell=cell,inputs=rnn_inputs,initial_state=init_state)\n",
    "\n",
    "#Predictions\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W',[STATE_SIZE,NUM_CLASSES])\n",
    "    b = tf.get_variable('b',[NUM_CLASSES],initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "logits = tf.reshape(tf.matmul(tf.reshape(rnn_outputs,shape=[-1,STATE_SIZE]),W) + b,shape=[BATCH_SIZE,NUM_STEPS,NUM_CLASSES])\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "#Loss\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "#Optimizer\n",
    "train_step = tf.train.AdagradOptimizer(LEARNING_RATE).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(num_epochs):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        \n",
    "        for idx,epoch in enumerate(gen_epochs(num_epochs,BATCH_SIZE,NUM_STEPS)):\n",
    "            training_loss = 0\n",
    "            training_state = np.random.normal(size=(BATCH_SIZE,STATE_SIZE)) #Random initiate state\n",
    "        \n",
    "            print('Epoch %d'%idx)\n",
    "            \n",
    "            for step,(X,Y) in enumerate(epoch):\n",
    "                \n",
    "                feed_dict = {x:X,y:Y,init_state:training_state}\n",
    "                tr_losses,training_loss_,training_state,_ = sess.run([losses,total_loss,final_state,train_step],feed_dict)\n",
    "                training_loss += training_loss_\n",
    "                \n",
    "                if(step % 100 == 0 and step > 0):\n",
    "                    print('Average loss at step %d for last 250 steps: %.3f'%(step,training_loss/100))\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "                \n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Average loss at step 100 for last 250 steps: 0.562\n",
      "Average loss at step 200 for last 250 steps: 0.499\n",
      "Average loss at step 300 for last 250 steps: 0.481\n",
      "Epoch 1\n",
      "Average loss at step 100 for last 250 steps: 0.501\n",
      "Average loss at step 200 for last 250 steps: 0.502\n",
      "Average loss at step 300 for last 250 steps: 0.477\n",
      "Epoch 2\n",
      "Average loss at step 100 for last 250 steps: 0.483\n",
      "Average loss at step 200 for last 250 steps: 0.481\n",
      "Average loss at step 300 for last 250 steps: 0.482\n",
      "Epoch 3\n",
      "Average loss at step 100 for last 250 steps: 0.479\n",
      "Average loss at step 200 for last 250 steps: 0.477\n",
      "Average loss at step 300 for last 250 steps: 0.481\n",
      "Epoch 4\n",
      "Average loss at step 100 for last 250 steps: 0.502\n",
      "Average loss at step 200 for last 250 steps: 0.473\n",
      "Average loss at step 300 for last 250 steps: 0.483\n",
      "Epoch 5\n",
      "Average loss at step 100 for last 250 steps: 0.473\n",
      "Average loss at step 200 for last 250 steps: 0.480\n",
      "Average loss at step 300 for last 250 steps: 0.467\n",
      "Epoch 6\n",
      "Average loss at step 100 for last 250 steps: 0.480\n",
      "Average loss at step 200 for last 250 steps: 0.477\n",
      "Average loss at step 300 for last 250 steps: 0.478\n",
      "Epoch 7\n",
      "Average loss at step 100 for last 250 steps: 0.470\n",
      "Average loss at step 200 for last 250 steps: 0.487\n",
      "Average loss at step 300 for last 250 steps: 0.467\n",
      "Epoch 8\n",
      "Average loss at step 100 for last 250 steps: 0.481\n",
      "Average loss at step 200 for last 250 steps: 0.467\n",
      "Average loss at step 300 for last 250 steps: 0.464\n",
      "Epoch 9\n",
      "Average loss at step 100 for last 250 steps: 0.478\n",
      "Average loss at step 200 for last 250 steps: 0.466\n",
      "Average loss at step 300 for last 250 steps: 0.479\n"
     ]
    }
   ],
   "source": [
    "training_losses = train_network(num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOSzMzCzJYWFmZkkOCzMzS3JYmJlZ0v8HnTvw9+w4+Y8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x226ac30b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss')\n",
    "plt.scatter(x=np.arange(0,len(training_losses)),y=training_losses)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
