{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLSTM\n",
    "\n",
    "In this Notebook we will build upon our vanilla RNN by learning how to use Tensorflowâ€™s scan and dynamic_rnn models, upgrading the RNN cell and stacking multiple RNNs, and adding dropout. We will then use our upgraded RNN to generate some text, character by character.\n",
    "\n",
    "<a href=\"https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii\">[Ref]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import NLP_Utils as nlp\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data\n",
    "FILENAME = 'tiny_shakespeare.txt'\n",
    "\n",
    "#Cell\n",
    "STATE_SIZE = 100\n",
    "\n",
    "#Batches & Vocab\n",
    "BATCH_SIZE = 32 #Default: 64\n",
    "NUM_STEPS = 80 #Default: 50\n",
    "OVERLAP = NUM_STEPS #Default 25\n",
    "VOCAB_STRATEGY = 'all'\n",
    "\n",
    "#Test\n",
    "TEST_SIZE = 300 #Default: 300\n",
    "\n",
    "#Model\n",
    "NUM_EPOCHS = 20\n",
    "NUM_LAYERS = 3\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@params:\n",
      "[filename : \"tiny_shakespeare.txt\"]\n",
      "[text : \"First Citi\"]\n",
      "[text_size : 1115394]\n",
      "[vocab : ['c', 'x', 'W', 'y', 'z']]\n",
      "[vocab_size : 65]\n",
      "[mode : dense]\n",
      "[batch_size : 32]\n",
      "[seq_length : 80]\n",
      "[overlap : 80]\n",
      "[num_batches : 435]\n"
     ]
    }
   ],
   "source": [
    "batchManager = nlp.BatchManager()\n",
    "batchManager.set_params(FILENAME,BATCH_SIZE,NUM_STEPS,OVERLAP,VOCAB_STRATEGY,mode='dense')\n",
    "batchManager.get_params()\n",
    "\n",
    "NUM_BATCHES = batchManager._num_batches\n",
    "VOCAB_SIZE = batchManager._vocab_size\n",
    "NUM_CLASSES = VOCAB_SIZE\n",
    "\n",
    "DISPLAY_FREQ = NUM_BATCHES//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus has 1115394 characters\n",
      "Configuration:\n",
      "[batch_size : 32]\n",
      "[seq_length : 80]\n",
      "[overlap : 80]\n",
      "\n",
      "The current configuration gives us 435 batches of 32 observations each one looking 80 steps in the past and overlapping 0 steps\n"
     ]
    }
   ],
   "source": [
    "batchManager.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open(FILENAME,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    print(\"Data length:\", len(raw_data))\n",
    "\n",
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_multilayer_lstm_graph_with_dynamic_rnn(state_size,num_classes,batch_size,num_steps,num_layers,learning_rate):\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    #Placholders\n",
    "    x = tf.placeholder(tf.int32,[batch_size,num_steps],name='input_placeholder')\n",
    "    y= tf.placeholder(tf.int32,[batch_size,num_steps],name='labels_placeholder')\n",
    "    \n",
    "    embeddings = tf.get_variable('embedding_matrix',[num_classes,state_size])\n",
    "    \n",
    "    #rnn inputs is a tensor of [batch_size, num_steps, state_size]\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings,x)\n",
    "    \n",
    "    #RNN Cell\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size,state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    rnn_outputs,final_state = tf.nn.dynamic_rnn(cell,rnn_inputs,initial_state=init_state)\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W',[state_size,num_classes])\n",
    "        b = tf.get_variable('b',[num_classes],initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "    #Reshape rnn_inputs and y so we can get the logits in a single matmul\n",
    "    rnn_outputs = tf.reshape(rnn_outputs,[-1,state_size])\n",
    "    y_reshaped = tf.reshape(y,[-1])\n",
    "        \n",
    "    #Logits & predictions\n",
    "    logits = tf.matmul(rnn_outputs,W) + b\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    ret_dict = dict(x=x, y=y, init_state=init_state, final_state=final_state, \n",
    "                    total_loss=total_loss, train_step=train_step,preds=predictions,saver=saver)\n",
    "    \n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(g,num_epochs,num_steps,batch_size,save='saves/last_model'):\n",
    "    step = 0\n",
    "    batchManager.generate_batches()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        training_losses = []\n",
    "        \n",
    "        for X,Y,epoch in batchManager.generate_batches(num_epochs):\n",
    "                        \n",
    "            training_loss = 0\n",
    "            training_state = None\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            feed_dict = {g['x']:X,g['y']:Y}\n",
    "\n",
    "            if training_state is not None:\n",
    "                feed_dict[g['init_state']] = training_state\n",
    "            training_loss_, training_state_, _ = sess.run([g['total_loss'],g['final_state'],g['train_step']],feed_dict)\n",
    "\n",
    "            training_loss += training_loss_\n",
    "                \n",
    "            \n",
    "            if(step % DISPLAY_FREQ == 0):\n",
    "                print('Epoch %d - At step %d average training loss: %.3f'%(epoch,step,training_loss/step))\n",
    "                training_losses.append(training_loss/step)\n",
    "            \n",
    "        step = 0\n",
    "        if isinstance(save,str):\n",
    "            g['saver'].save(sess,save)\n",
    "            \n",
    "        return training_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_characters(g,checkpoint,num_chars,prompt='A',pick_top_chars = None):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        g['saver'].restore(sess,checkpoint)\n",
    "        \n",
    "        state = None\n",
    "        current_char = batchManager.vocab_encode(prompt)[0]\n",
    "        chars = [current_char]\n",
    "        \n",
    "        for i in range(num_chars):\n",
    "            if state is not None:\n",
    "                feed_dict = {g['x']:[[current_char]], g['init_state']:state}\n",
    "            else:\n",
    "                feed_dict = {g['x']:[[current_char]]}\n",
    "                \n",
    "            preds,state = sess.run([g['preds'],g['final_state']],feed_dict)\n",
    "            \n",
    "            if(pick_top_chars is not None):\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size,1,p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size,1,p=np.squeeze(preds))[0]\n",
    "                \n",
    "            chars.append(current_char)\n",
    "        \n",
    "    chars = batchManager.vocab_decode(chars)\n",
    "    print(\"\".join(chars))\n",
    "    return (\"\".join(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_multilayer_lstm_graph_with_dynamic_rnn(STATE_SIZE,NUM_CLASSES,BATCH_SIZE,NUM_STEPS,NUM_LAYERS,LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - At step 43 average training loss: 0.094\n",
      "Epoch 0 - At step 86 average training loss: 0.041\n",
      "Epoch 0 - At step 129 average training loss: 0.027\n",
      "Epoch 0 - At step 172 average training loss: 0.020\n",
      "Epoch 0 - At step 215 average training loss: 0.015\n",
      "Epoch 0 - At step 258 average training loss: 0.013\n",
      "Epoch 0 - At step 301 average training loss: 0.011\n",
      "Epoch 0 - At step 344 average training loss: 0.010\n",
      "Epoch 0 - At step 387 average training loss: 0.009\n",
      "Epoch 0 - At step 430 average training loss: 0.008\n",
      "Epoch 1 - At step 473 average training loss: 0.007\n",
      "Epoch 1 - At step 516 average training loss: 0.007\n",
      "Epoch 1 - At step 559 average training loss: 0.006\n",
      "Epoch 1 - At step 602 average training loss: 0.006\n",
      "Epoch 1 - At step 645 average training loss: 0.005\n",
      "Epoch 1 - At step 688 average training loss: 0.005\n",
      "Epoch 1 - At step 731 average training loss: 0.004\n",
      "Epoch 1 - At step 774 average training loss: 0.004\n",
      "Epoch 1 - At step 817 average training loss: 0.004\n",
      "Epoch 1 - At step 860 average training loss: 0.004\n",
      "Epoch 2 - At step 903 average training loss: 0.004\n",
      "Epoch 2 - At step 946 average training loss: 0.003\n",
      "Epoch 2 - At step 989 average training loss: 0.003\n",
      "Epoch 2 - At step 1032 average training loss: 0.003\n",
      "Epoch 2 - At step 1075 average training loss: 0.003\n",
      "Epoch 2 - At step 1118 average training loss: 0.003\n",
      "Epoch 2 - At step 1161 average training loss: 0.003\n",
      "Epoch 2 - At step 1204 average training loss: 0.003\n",
      "Epoch 2 - At step 1247 average training loss: 0.003\n",
      "Epoch 2 - At step 1290 average training loss: 0.003\n",
      "Epoch 3 - At step 1333 average training loss: 0.003\n",
      "Epoch 3 - At step 1376 average training loss: 0.002\n",
      "Epoch 3 - At step 1419 average training loss: 0.002\n",
      "Epoch 3 - At step 1462 average training loss: 0.002\n",
      "Epoch 3 - At step 1505 average training loss: 0.002\n",
      "Epoch 3 - At step 1548 average training loss: 0.002\n",
      "Epoch 3 - At step 1591 average training loss: 0.002\n",
      "Epoch 3 - At step 1634 average training loss: 0.002\n",
      "Epoch 3 - At step 1677 average training loss: 0.002\n",
      "Epoch 3 - At step 1720 average training loss: 0.002\n",
      "Epoch 4 - At step 1763 average training loss: 0.002\n",
      "Epoch 4 - At step 1806 average training loss: 0.002\n",
      "Epoch 4 - At step 1849 average training loss: 0.002\n",
      "Epoch 4 - At step 1892 average training loss: 0.002\n",
      "Epoch 4 - At step 1935 average training loss: 0.002\n",
      "Epoch 4 - At step 1978 average training loss: 0.002\n",
      "Epoch 4 - At step 2021 average training loss: 0.002\n",
      "Epoch 4 - At step 2064 average training loss: 0.001\n",
      "Epoch 4 - At step 2107 average training loss: 0.001\n",
      "Epoch 4 - At step 2150 average training loss: 0.001\n",
      "Epoch 5 - At step 2193 average training loss: 0.001\n",
      "Epoch 5 - At step 2236 average training loss: 0.001\n",
      "Epoch 5 - At step 2279 average training loss: 0.001\n",
      "Epoch 5 - At step 2322 average training loss: 0.001\n",
      "Epoch 5 - At step 2365 average training loss: 0.001\n",
      "Epoch 5 - At step 2408 average training loss: 0.001\n",
      "Epoch 5 - At step 2451 average training loss: 0.001\n",
      "Epoch 5 - At step 2494 average training loss: 0.001\n",
      "Epoch 5 - At step 2537 average training loss: 0.001\n",
      "Epoch 5 - At step 2580 average training loss: 0.001\n",
      "Epoch 6 - At step 2623 average training loss: 0.001\n",
      "Epoch 6 - At step 2666 average training loss: 0.001\n",
      "Epoch 6 - At step 2709 average training loss: 0.001\n",
      "Epoch 6 - At step 2752 average training loss: 0.001\n",
      "Epoch 6 - At step 2795 average training loss: 0.001\n",
      "Epoch 6 - At step 2838 average training loss: 0.001\n",
      "Epoch 6 - At step 2881 average training loss: 0.001\n",
      "Epoch 6 - At step 2924 average training loss: 0.001\n",
      "Epoch 6 - At step 2967 average training loss: 0.001\n",
      "Epoch 6 - At step 3010 average training loss: 0.001\n",
      "Epoch 7 - At step 3053 average training loss: 0.001\n",
      "Epoch 7 - At step 3096 average training loss: 0.001\n",
      "Epoch 7 - At step 3139 average training loss: 0.001\n",
      "Epoch 7 - At step 3182 average training loss: 0.001\n",
      "Epoch 7 - At step 3225 average training loss: 0.001\n",
      "Epoch 7 - At step 3268 average training loss: 0.001\n",
      "Epoch 7 - At step 3311 average training loss: 0.001\n",
      "Epoch 7 - At step 3354 average training loss: 0.001\n",
      "Epoch 7 - At step 3397 average training loss: 0.001\n",
      "Epoch 7 - At step 3440 average training loss: 0.001\n",
      "Epoch 8 - At step 3483 average training loss: 0.001\n",
      "Epoch 8 - At step 3526 average training loss: 0.001\n",
      "Epoch 8 - At step 3569 average training loss: 0.001\n",
      "Epoch 8 - At step 3612 average training loss: 0.001\n",
      "Epoch 8 - At step 3655 average training loss: 0.001\n",
      "Epoch 8 - At step 3698 average training loss: 0.001\n",
      "Epoch 8 - At step 3741 average training loss: 0.001\n",
      "Epoch 8 - At step 3784 average training loss: 0.001\n",
      "Epoch 8 - At step 3827 average training loss: 0.001\n",
      "Epoch 8 - At step 3870 average training loss: 0.001\n",
      "Epoch 8 - At step 3913 average training loss: 0.001\n",
      "Epoch 9 - At step 3956 average training loss: 0.001\n",
      "Epoch 9 - At step 3999 average training loss: 0.001\n",
      "Epoch 9 - At step 4042 average training loss: 0.001\n",
      "Epoch 9 - At step 4085 average training loss: 0.001\n",
      "Epoch 9 - At step 4128 average training loss: 0.001\n",
      "Epoch 9 - At step 4171 average training loss: 0.001\n",
      "Epoch 9 - At step 4214 average training loss: 0.001\n",
      "Epoch 9 - At step 4257 average training loss: 0.001\n",
      "Epoch 9 - At step 4300 average training loss: 0.001\n",
      "Epoch 9 - At step 4343 average training loss: 0.001\n",
      "Epoch 10 - At step 4386 average training loss: 0.001\n",
      "Epoch 10 - At step 4429 average training loss: 0.001\n",
      "Epoch 10 - At step 4472 average training loss: 0.001\n",
      "Epoch 10 - At step 4515 average training loss: 0.001\n",
      "Epoch 10 - At step 4558 average training loss: 0.001\n",
      "Epoch 10 - At step 4601 average training loss: 0.001\n",
      "Epoch 10 - At step 4644 average training loss: 0.001\n",
      "Epoch 10 - At step 4687 average training loss: 0.001\n",
      "Epoch 10 - At step 4730 average training loss: 0.001\n",
      "Epoch 10 - At step 4773 average training loss: 0.001\n",
      "Epoch 11 - At step 4816 average training loss: 0.001\n",
      "Epoch 11 - At step 4859 average training loss: 0.001\n",
      "Epoch 11 - At step 4902 average training loss: 0.001\n",
      "Epoch 11 - At step 4945 average training loss: 0.001\n",
      "Epoch 11 - At step 4988 average training loss: 0.001\n",
      "Epoch 11 - At step 5031 average training loss: 0.000\n",
      "Epoch 11 - At step 5074 average training loss: 0.000\n",
      "Epoch 11 - At step 5117 average training loss: 0.000\n",
      "Epoch 11 - At step 5160 average training loss: 0.000\n",
      "Epoch 11 - At step 5203 average training loss: 0.000\n",
      "Epoch 12 - At step 5246 average training loss: 0.000\n",
      "Epoch 12 - At step 5289 average training loss: 0.000\n",
      "Epoch 12 - At step 5332 average training loss: 0.000\n",
      "Epoch 12 - At step 5375 average training loss: 0.000\n",
      "Epoch 12 - At step 5418 average training loss: 0.000\n",
      "Epoch 12 - At step 5461 average training loss: 0.000\n",
      "Epoch 12 - At step 5504 average training loss: 0.000\n",
      "Epoch 12 - At step 5547 average training loss: 0.000\n",
      "Epoch 12 - At step 5590 average training loss: 0.000\n",
      "Epoch 12 - At step 5633 average training loss: 0.000\n",
      "Epoch 13 - At step 5676 average training loss: 0.000\n",
      "Epoch 13 - At step 5719 average training loss: 0.000\n",
      "Epoch 13 - At step 5762 average training loss: 0.000\n",
      "Epoch 13 - At step 5805 average training loss: 0.000\n",
      "Epoch 13 - At step 5848 average training loss: 0.000\n",
      "Epoch 13 - At step 5891 average training loss: 0.000\n",
      "Epoch 13 - At step 5934 average training loss: 0.000\n",
      "Epoch 13 - At step 5977 average training loss: 0.000\n",
      "Epoch 13 - At step 6020 average training loss: 0.000\n",
      "Epoch 13 - At step 6063 average training loss: 0.000\n",
      "Epoch 14 - At step 6106 average training loss: 0.000\n",
      "Epoch 14 - At step 6149 average training loss: 0.000\n",
      "Epoch 14 - At step 6192 average training loss: 0.000\n",
      "Epoch 14 - At step 6235 average training loss: 0.000\n",
      "Epoch 14 - At step 6278 average training loss: 0.000\n",
      "Epoch 14 - At step 6321 average training loss: 0.000\n",
      "Epoch 14 - At step 6364 average training loss: 0.000\n",
      "Epoch 14 - At step 6407 average training loss: 0.000\n",
      "Epoch 14 - At step 6450 average training loss: 0.000\n",
      "Epoch 14 - At step 6493 average training loss: 0.000\n",
      "Epoch 15 - At step 6536 average training loss: 0.000\n",
      "Epoch 15 - At step 6579 average training loss: 0.000\n",
      "Epoch 15 - At step 6622 average training loss: 0.000\n",
      "Epoch 15 - At step 6665 average training loss: 0.000\n",
      "Epoch 15 - At step 6708 average training loss: 0.000\n",
      "Epoch 15 - At step 6751 average training loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - At step 6794 average training loss: 0.000\n",
      "Epoch 15 - At step 6837 average training loss: 0.000\n",
      "Epoch 15 - At step 6880 average training loss: 0.000\n",
      "Epoch 15 - At step 6923 average training loss: 0.000\n",
      "Epoch 16 - At step 6966 average training loss: 0.000\n",
      "Epoch 16 - At step 7009 average training loss: 0.000\n",
      "Epoch 16 - At step 7052 average training loss: 0.000\n",
      "Epoch 16 - At step 7095 average training loss: 0.000\n",
      "Epoch 16 - At step 7138 average training loss: 0.000\n",
      "Epoch 16 - At step 7181 average training loss: 0.000\n",
      "Epoch 16 - At step 7224 average training loss: 0.000\n",
      "Epoch 16 - At step 7267 average training loss: 0.000\n",
      "Epoch 16 - At step 7310 average training loss: 0.000\n",
      "Epoch 16 - At step 7353 average training loss: 0.000\n",
      "Epoch 17 - At step 7396 average training loss: 0.000\n",
      "Epoch 17 - At step 7439 average training loss: 0.000\n",
      "Epoch 17 - At step 7482 average training loss: 0.000\n",
      "Epoch 17 - At step 7525 average training loss: 0.000\n",
      "Epoch 17 - At step 7568 average training loss: 0.000\n",
      "Epoch 17 - At step 7611 average training loss: 0.000\n",
      "Epoch 17 - At step 7654 average training loss: 0.000\n",
      "Epoch 17 - At step 7697 average training loss: 0.000\n",
      "Epoch 17 - At step 7740 average training loss: 0.000\n",
      "Epoch 17 - At step 7783 average training loss: 0.000\n",
      "Epoch 17 - At step 7826 average training loss: 0.000\n",
      "Epoch 18 - At step 7869 average training loss: 0.000\n",
      "Epoch 18 - At step 7912 average training loss: 0.000\n",
      "Epoch 18 - At step 7955 average training loss: 0.000\n",
      "Epoch 18 - At step 7998 average training loss: 0.000\n",
      "Epoch 18 - At step 8041 average training loss: 0.000\n",
      "Epoch 18 - At step 8084 average training loss: 0.000\n",
      "Epoch 18 - At step 8127 average training loss: 0.000\n",
      "Epoch 18 - At step 8170 average training loss: 0.000\n",
      "Epoch 18 - At step 8213 average training loss: 0.000\n",
      "Epoch 18 - At step 8256 average training loss: 0.000\n",
      "Epoch 19 - At step 8299 average training loss: 0.000\n",
      "Epoch 19 - At step 8342 average training loss: 0.000\n",
      "Epoch 19 - At step 8385 average training loss: 0.000\n",
      "Epoch 19 - At step 8428 average training loss: 0.000\n",
      "Epoch 19 - At step 8471 average training loss: 0.000\n",
      "Epoch 19 - At step 8514 average training loss: 0.000\n",
      "Epoch 19 - At step 8557 average training loss: 0.000\n",
      "Epoch 19 - At step 8600 average training loss: 0.000\n",
      "Epoch 19 - At step 8643 average training loss: 0.000\n",
      "Epoch 19 - At step 8686 average training loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "training_losses = train_network(g,NUM_EPOCHS,NUM_STEPS,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_multilayer_lstm_graph_with_dynamic_rnn(STATE_SIZE,NUM_CLASSES,batch_size=1,num_steps=1,num_layers=NUM_LAYERS,learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saves/last_model\n",
      "Asd ant angen hase thint\n",
      "I the the to tout at, the me asd.\n",
      "\n",
      "ANELTI:\n",
      "I thor wount thas sat ort as soush the\n",
      "The wo with wish thus hit thing sour thim, asd and.\n",
      "\n",
      "ANOOO:\n",
      "Is shett tho te mather, hite are he the sond,\n",
      "Wheth thin athar to mertest sive,\n",
      "Thant sharl sond me han angent there thar ant sontile hind arl,\n",
      "Ang that to shit withe, sour mesd and hict shang the thos ant masese sonten arlire as to sottere and,\n",
      "Bons to sounthess hand afse so the shale to shour his\n",
      "Thim ther hos thee horss and the thour thar the\n",
      "shut asd whan ant me and her to cittis tering and matter the sher, mes sonss.\n",
      "\n",
      "INTI I IRIO:\n",
      "Woul sort mere asd,\n",
      "Ang and. Insest to and so muul to toud thee sorder.\n",
      "\n",
      "IONI:\n",
      "A sons mo theud mour a sontithes werle to and, the ham mat thes.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Asd ant angen hase thint\\nI the the to tout at, the me asd.\\n\\nANELTI:\\nI thor wount thas sat ort as soush the\\nThe wo with wish thus hit thing sour thim, asd and.\\n\\nANOOO:\\nIs shett tho te mather, hite are he the sond,\\nWheth thin athar to mertest sive,\\nThant sharl sond me han angent there thar ant sontile hind arl,\\nAng that to shit withe, sour mesd and hict shang the thos ant masese sonten arlire as to sottere and,\\nBons to sounthess hand afse so the shale to shour his\\nThim ther hos thee horss and the thour thar the\\nshut asd whan ant me and her to cittis tering and matter the sher, mes sonss.\\n\\nINTI I IRIO:\\nWoul sort mere asd,\\nAng and. Insest to and so muul to toud thee sorder.\\n\\nIONI:\\nA sons mo theud mour a sontithes werle to and, the ham mat thes.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_characters(g,checkpoint='saves/last_model',num_chars=750,prompt='A',pick_top_chars=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
